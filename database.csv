Title,Category,PublicationDate,ArXiv Link,GitHub,ProjectPage,Connect,Citation,Publication Date,Impact,Foundation,Property,Model,ID,ImageURL,In a nut shell,SemanticScholar,image
"LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report",LoRA,2405,https://arxiv.org/abs/2405.00732,,,,,,,,,,531,,,,
OpenELM: An Efficient Language Model Family with Open Training and Inference Framework,"Apple, LLM, Open-source",2404,https://arxiv.org/pdf/2404.14619,,,,,,,,,,546,,,,
Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs,Apple,2404,https://arxiv.org/pdf/2404.05719,,,,,,,,,,541,,,,
Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models,Apple,2404,https://arxiv.org/pdf/2404.07973,,,,,,,,,,540,,,,
PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning,"Caption, Video",2404,https://arxiv.org/pdf/2404.16994,,,,,,,,,,520,,,,
RecurrentGemma: Moving Past Transformers for Efficient Open Language Models,"Open-source, SLM",2404,https://arxiv.org/abs/2404.07839,,,,,,,,,,512,,,,
Many-Shot In-Context Learning,"In-Context-Learning, Many-Shot, Reasoning",2404,https://arxiv.org/abs/2404.11018,,,,,,,,,,507,,,,
Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models,VisualPrompt,2404,https://arxiv.org/abs/2404.07973,,,,,,,,,,479,,,,
Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention,Scaling,2404,https://arxiv.org/abs/2404.07143,,,,,,,,,,478,,,,
MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding,Video,2404,https://arxiv.org/abs/2404.05726,,,,,,,,,,475,,,,
"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","Apple, VLM",2403,https://arxiv.org/pdf/2403.09611,,,,,,,,,,539,,,,
ReALM: Reference Resolution As Language Modeling,"Apple, LLM",2403,https://arxiv.org/pdf/2403.20329,,,,,,,,,,538,,,,
LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning,"Robot, TAMP",2403,https://arxiv.org/pdf/2403.11552,,,,,,,,,,532,,,,
Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models,VLM,2403,https://arxiv.org/pdf/2403.18814.pdf,,,,,,,,,,515,,,,
ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models,"Affordance, Segmentation",2403,https://arxiv.org/abs/2403.11289,,,,,,,,,,513,,,,
MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting,VisualPrompt,2403,https://arxiv.org/abs/2403.03174,,,,,,,,,,483,,,,
Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,"Sora, Text-to-Video",2403,https://arxiv.org/abs/2403.13248,,,,,,,,,,468,https://arxiv.org/html/2403.13248v1/x8.png,,,
InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding,"ViFM, Video",2403,https://arxiv.org/abs/2403.15377,https://github.com/OpenGVLab/InternVideo2/,,,,,,,,,466,https://media.licdn.com/dms/image/D4E12AQEmYG1sedv9hg/article-cover_image-shrink_720_1280/0/1711664275358?e=2147483647&v=beta&t=rHsko-hNau4Mb0v8CEtPlaDzPsNJwkCI31w9p3AxEVk,,,
Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation,Tex2Img,2403,https://arxiv.org/abs/2403.16990,,,,,,,,,,463,https://omer11a.github.io/bounded-attention/static/images/xl_results/8.jpg,,,
"DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies",Robot,2403,https://arxiv.org/abs/2403.07832,,,,,,,,,,461,,,,
Explorative Inbetweening of Time and Space,Temporal,2403,https://arxiv.org/abs/2403.14611,,,,,,,,,,460,,,,
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity,RAG,2403,https://arxiv.org/abs/2403.14403,,,,,,,,,,459,https://pbs.twimg.com/media/GJTnvhBbsAAT8Po?format=jpg&name=4096x4096,,,
AIOS: LLM Agent Operating System,Agent,2403,https://arxiv.org/abs/2403.16971,,,,,,,,,,457,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSXFO2F4NIQ-EAzQYBBaPAYJQ0fHX64YfobmDIW5G6vsg&s,,,
RAFT: Adapting Language Model to Domain Specific RAG,RAG,2403,https://arxiv.org/abs/2403.10131,,,,,,,,,,456,,,,
Can large language models explore in-context?,In-Context-Learning,2403,https://arxiv.org/pdf/2403.15371,,,,,,,,,,448,,,,
OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,"Agent, Embodied, Robot",2403,https://arxiv.org/pdf/2403.03017,,,,,,,,,,441,,,,
3D Diffusion Policy,"Diffusion, Robot",2403,https://arxiv.org/abs/2403.03954,,,,,,,,,,354,,,,
VisionLLaMA: A Unified LLaMA Interface for Vision Tasks,"Foundation, LLaMA, Vision",2403,https://arxiv.org/pdf/2403.00522,,,,,,,,,,321,https://media.licdn.com/dms/image/D4E12AQEs7aGgeapSdw/article-inline_image-shrink_1000_1488/0/1709592515484?e=2147483647&v=beta&t=gMRatq5gkN4ofk2WK5-jbVEWVqq0tgloCTRBJY7QiI8,,,
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models,"Agent, Diffusion, Speech",2403,https://arxiv.org/pdf/2403.03100,,,,,,,,,,320,,,,
Design2Code: How Far Are We From Automating Front-End Engineering?,"Code-LLM, Front-End",2403,https://arxiv.org/pdf/2403.03163,,,,,,,,,,314,,,,
RT-H: Action Hierarchies Using Language,"Natural-Language-as-Polices, Robot",2403,https://arxiv.org/abs/2403.01823,,,,,,,,,,311,,,,
Resonance RoPE: Improving Context Length Generalization of Large Language Models,"Context-Window, Reasoning, RoPE, Scaling",2403,https://arxiv.org/pdf/2403.00071,,,,,,,,,,270,,,,
Natural Language as Policies: Reasoning for Coordinate-Level Embodied Control with LLMs,"Embodied, Reasoning, Robot",2403,https://arxiv.org/abs/2403.13801,https://github.com/shure-dev/NLaP,https://natural-language-as-policies.github.io/,,,2024/03/20,,,,NLaP,217,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRfbEXAvxjSGt1J9ziEDKyhkxFQq1-U1S3z2splPQn_YA&s,,,
Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication,Reasoning,2402,https://arxiv.org/abs/2402.18439,,,,,,,,,,509,,,,
RAG-Fusion: a New Take on Retrieval-Augmented Generation,RAG,2402,https://arxiv.org/abs/2402.03367,,,,,,,,,,454,,,,
S-Agents: Self-organizing Agents in Open-ended Environment,"Agent, Minecraft",2402,https://arxiv.org/pdf/2402.04578,,,,,,,,,,404,,,,
InCoRo: In-Context Learning for Robotics Control with Feedback Loops,"Feedback, In-Context-Learning, Robot",2402,https://arxiv.org/abs/2402.05188,,,,,,,,,,403,,,,
A Survey on Data Selection for Language Models,"Datatset, LLM, Survey",2402,https://arxiv.org/pdf/2402.05123,,,,,,,,,,391,,,,
A Survey on Data Selection for LLM Instruction Tuning,"Instruction-Turning, Survey",2402,https://arxiv.org/pdf/2402.05123,,,,,,,,,,388,,,,
RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation,"Code-as-Policies, Robot",2402,https://arxiv.org/pdf/2402.14623,,,,,,,,,,358,,,,
Real-World Robot Applications of Foundation Models: A Review,"Robot, Survey",2402,https://arxiv.org/pdf/2402.05741,,,,,,,,,,357,,,,
A Closer Look at the Limitations of Instruction Tuning,"Instruction-Turning, Survey",2402,https://arxiv.org/abs/2402.05119,,,,,,,,,,329,,,,
Secrets of RLHF in Large Language Models Part I: PPO,"PPO, RLHF, Reinforcement-Learning",2402,https://arxiv.org/abs/2402.01030,,,,,2024/02/01,,,,,323,,,,
EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions,"Audio2Video, Diffusion, Generation, Video",2402,https://arxiv.org/pdf/2402.17485,,,,,,,,,,322,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSvx9wXPZF3MJFzjSO1GdIuR4ist9V2VcULNT5Sk2O78A&s,,,
Executable Code Actions Elicit Better LLM Agents,"Apple, Code-as-Policies, Robot",2402,https://arxiv.org/pdf/2402.01030,,,,,,,,,,317,,,,
ScreenAgent: A Vision Language Model-driven Computer Control Agent,Agent,2402,https://arxiv.org/pdf/2402.07945,,,,,,,,,,315,,,,
PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs,Robot,2402,https://arxiv.org/abs/2402.07872,,,,,,,,,,309,,,,
Diffusion World Model,World-model,2402,https://arxiv.org/abs/2402.03570,,,,,,,,,,308,,,,
RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents,"Agent, Memory, RAG, Robot",2402,https://arxiv.org/abs/2402.03610,,,,,2024/02/06,,,,,303,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ_QDJp6ApwRN6jlDLF7cOSVfD1TmXT_ZS72sniqN5caw&s,,,
Executable Code Actions Elicit Better LLM Agents,"Agent, Code-as-Policies",2402,https://arxiv.org/abs/2402.01030,,,,,2024/01/24,,,,,294,,,,
RoboCodeX:Multi-modal Code Generation forRobotic Behavior Synthesis,"Code-as-Policies, PersonalCitation, Robot",2402,https://arxiv.org/abs/2402.16117,,,,,,,,,,290,,,,
Large Multimodal Agents: A Survey,"Agent, Survey",2402,https://arxiv.org/pdf/2402.15116,,,,,,,,,,283,,,,
Large World Model,"VLM, World-model",2402,https://arxiv.org/abs/2402.08268,,,,,,,,,,268,,,,
Code as Reward: Empowering Reinforcement Learning with VLMs,"Code-as-Policies, Reinforcement-Learning, Reward",2402,https://arxiv.org/abs/2402.04764,,,,,,,,,,259,,,,
Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting,"Robot, Zero-shot",2402,https://arxiv.org/pdf/2402.19249,,,,,,,,,,255,,,,
OS-Copilot: Towards Generalist Computer Agents with Self-Improvement,"Agent, Web",2402,https://arxiv.org/abs/2402.07456,,,,,,,,,,251,,,,
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Math, Reasoning",2402,https://arxiv.org/pdf/2402.03300,https://github.com/deepseek-ai/DeepSeek-Math,,,,,,,,,246,,,,
Chain-of-Thought Reasoning Without Prompting,Reasoning,2402,https://arxiv.org/abs/2402.10200,,,,,,,,,,234,,,,
Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots,"Robot, Zero-shot",2402,https://arxiv.org/pdf/2402.10329,,,,,,,,,,233,,,,
Learning to Learn Faster from Human Feedback with Language Model Predictive Control,"Feedback, Robot",2402,https://arxiv.org/pdf/2402.11450,,,,,,,,,,231,,,,
LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,"Context-Window, LLM, RoPE, Scaling",2402,https://arxiv.org/abs/2402.13753,,,,,,50,,,,228,https://preview.redd.it/longrope-extending-llm-context-window-beyond-2-million-v0-dkyu60td2dkc1.jpg?width=1109&format=pjpg&auto=webp&s=a59c7ac6c1d8360450ddc4e121ed1150f4793cf0,,,
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,"Agent, Web",2402,https://arxiv.org/abs/2402.17553,,,,,,,,,OmniACT,225,,,,
TinyLLaVA: A Framework of Small-scale Large Multimodal Models,"LLaVA, VLM",2402,https://arxiv.org/abs/2402.14289,,,,,,,,,,223,https://lh7-us.googleusercontent.com/GbP7_I6N10hSV_4x6XVddgzApc1pCz2PylfJ28soBlWGt42WLS_UYiZZtBG_uHMqrFrjet0eZVGiTPetBaJRoeJ-1y68et9ek9TegiEOO2JVmy9VYbGwm_ofzJRTTYHGXfm51JMFVWZ798Bk3ERQqjs,,,
The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,"LLM, Quantization",2402,https://arxiv.org/abs/2402.17764,,,,,,70,,,,219,,,,
Reasoning Grasping via Multimodal Large Language Model,"Perception, Reasoning, Robot",2402,https://arxiv.org/abs/2402.06798,,,,,,,,,,215,,,,
World Model on Million-Length Video And Language With RingAttention,"Text-to-Image, World-model",2402,https://arxiv.org/abs/2402.08268,,,,,,,,,,206,,,,
Self-Discover: Large Language Models Self-Compose Reasoning Structures,Reasoning,2402,https://arxiv.org/pdf/2402.03620,,,,,,,,,,196,,,,
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,"Agent, Web",2402,https://arxiv.org/pdf/2402.05930,,,,,,,,,,195,,,,
Chain-of-Thought Reasoning Without Prompting,"Chain-of-Thought, Prompting",2402,https://arxiv.org/pdf/2402.10200,,,,,,,,,,194,,,,
A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,"Prompting, Survey",2402,https://arxiv.org/pdf/2402.07927,,,,,,,,,,193,,,,
An Interactive Agent Foundation Model,"Agent, End2End, Game, Robot",2402,https://arxiv.org/abs/2402.05929,,,,,,,,,,165,,,,
Mixtral of Experts,"Mixtral, MoE",2401,https://arxiv.org/pdf/2401.04088,,,,,,,,,,550,,,,
Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks,Segmentation,2401,https://arxiv.org/abs/2401.14159,,,,,,,,,,526,,,,
MoE-LLaVA: Mixture of Experts for Large Vision-Language Models,VLM,2401,https://arxiv.org/abs/2401.15947,https://github.com/PKU-YuanGroup/MoE-LLaVA,,,,,,,,,500,,,,
"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",RAG,2401,https://arxiv.org/abs/2401.08406,,,,,,,,,,476,,,,
Corrective Retrieval Augmented Generation,"CRAG, RAG",2401,https://arxiv.org/abs/2401.15884,,,,,,,,,,452,,,,
Chain-of-table: Evolving tables in the reasoning chain for table understanding,"Chain-of-Thought, Reasoning, Table",2401,https://arxiv.org/pdf/2401.04398,,,,,,,,,,435,,,,
OCI-Robotics: Object-Centric Instruction Augmentation for Robotic Manipulation,Robot,2401,https://arxiv.org/abs/2401.02814,,,,,,,,,,406,,,,
"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","Agent, Code-LLM, Code-as-Policies, Survey",2401,https://arxiv.org/abs/2401.00812,,,,5,,,,,,389,,,,
Understanding LLMs: A Comprehensive Overview from Training to Inference,"Survey, Training",2401,https://arxiv.org/pdf/2401.02038,,,,3,,,,,,387,,,,
SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents,"Agent, GUI",2401,https://arxiv.org/pdf/2401.10935,,,,,,,,,,380,,,,
Secrets of RLHF in Large Language Models Part II: Reward Modeling,RLHF,2401,https://arxiv.org/pdf/2401.06080,,,,,,,,,,337,,,,
The Impact of Reasoning Step Length on Large Language Models,Reasoning,2401,https://arxiv.org/pdf/2401.04925,,,,,,,,,,328,,,,
Agent AI: Surveying the Horizons of Multimodal Interaction,"Agent, Survey",2401,https://arxiv.org/pdf/2401.03568,,,,,,,,,,282,,,,
Generative Expressive Robot Behaviors using Large Language Models,Robot,2401,https://arxiv.org/pdf/2401.14673,,,,,,,,,,241,,,,
SliceGPT: Compress Large Language Models by Deleting Rows and Columns,"Quantization, Scaling",2401,https://arxiv.org/pdf/2401.15024,,,,,,,,,,240,,,,
"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs","Diffusion, Text-to-Image",2401,https://arxiv.org/pdf/2401.11708,,,,,,,,,,239,,,,
Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data,"Anything, Depth",2401,https://arxiv.org/pdf/2401.10891,,,,,,,,,,238,,,,
"GPT-4V(ision) is a Generalist Web Agent, if Grounded","Agent, GPT4, Web",2401,https://arxiv.org/pdf/2401.01614,,,,,,,,,,203,https://arxiv.org/html/2401.01614v1/x1.png,,,
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models,"Agent, Web",2401,https://arxiv.org/pdf/2401.13919,,,,,,,,,,192,https://d3i71xaburhd42.cloudfront.net/19261c6ad20c6c1e5585a8afcb88196173cbc8a6/2-Figure1-1.png,,,
OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics,Robot,2401,https://arxiv.org/pdf/2401.12202,,,,,,,,,,190,,,,
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,"Agent, Embodied, Robot",2401,https://arxiv.org/abs/2401.12963,,,,,,,,,,127,,,,
GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation,"3D, GPT4, VLM",2401,https://arxiv.org/abs/2401.04092,,,,,,,,,,122,,,,
Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding,"Chain-of-Thought, In-Context-Learning",2401,https://arxiv.org/abs/2401.04398,,,,,,,,,,121,,,,
FoMo Rewards: Can we cast foundation models as reward functions?,"Reinforcement-Learning, VIMA",2312,https://arxiv.org/pdf/2312.03881,,,,,,,,,,555,,,,
LLM in a flash: Efficient Large Language Model Inference with Limited Memory,"Apple, LLM",2312,https://arxiv.org/abs/2312.11514,,,,,,,,,,544,,,,
Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs,RAG,2312,https://arxiv.org/pdf/2312.05934.pdf,,,,,,,,,,497,,,,
Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations,"Math, PRM",2312,https://arxiv.org/abs/2312.08935,,,,,,,,,,493,,,,
Making Large Multimodal Models Understand Arbitrary Visual Prompts,VisualPrompt,2312,https://arxiv.org/abs/2312.00784,,,,,,,,,,481,,,,
"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",Agent,2312,https://arxiv.org/abs/2312.03815,,,,,,,,,,458,,,,
Mamba: Linear-Time Sequence Modeling with Selective State Spaces,"Context-Window, Foundation",2312,https://arxiv.org/pdf/2312.00752,,,,,,,,,,451,https://i.ytimg.com/vi/866SfiCHZ4o/maxresdefault.jpg,,,
Chain of Code: Reasoning with a Language Model-Augmented Code Emulator,"Chain-of-Thought, Code-as-Policies",2312,https://arxiv.org/abs/2312.04474,,,,,,,,,,442,,,,
Retrieval-Augmented Generation for Large Language Models: A Survey,"RAG, Survey",2312,https://arxiv.org/pdf/2312.10997,,,,,,,,,,433,,,,
Video Understanding with Large Language Models: A Survey,"Survey, Video",2312,https://arxiv.org/pdf/2312.17432,,,,,,,,,,431,,,,
Efficient Large Language Models: A Survey,Survey,2312,https://arxiv.org/abs/2312.03863,https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey,,,,,,,,,419,,,,
Lenna: Language Enhanced Reasoning Detection Assistant,"Perception, Reasoning",2312,https://arxiv.org/abs/2312.02433,,,,,,,,,,407,,,,
"""What’s important here?"": Opportunities and Challenges of Using LLMs in Retrieving Informatio from Web Interfaces","Agent, GUI, Web",2312,https://arxiv.org/pdf/2312.06147,,,,,,,,,,381,,,,
Can Language Agents Approach the Performance of RL? An Empirical Study On OpenAI Gym,"Gym, PPO, Reinforcement-Learning, Survey",2312,https://arxiv.org/pdf/2312.03290,,,,,,,,,,375,,,,
Segment and Caption Anything,"Anything, Caption, Perception, Segmentation",2312,https://arxiv.org/abs/2312.00869,,,,,,,,,,355,https://d3i71xaburhd42.cloudfront.net/339ec34efdccdf2bf43bb817ef7cab5058bfa2e7/1-Figure1-1.png,,,
Chain of Code: Reasoning with a Language Model-Augmented Code Emulator,"Code-as-Policies, Reasoning",2312,https://arxiv.org/pdf/2312.04474,,,,,,,,,,293,,,,
Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning,"Agent, Reasoning",2312,https://arxiv.org/pdf/2312.14878,,,,,,,,,,288,,,,
LARP: Language-Agent Role Play for Open-World Games,"Agent, Minecraft",2312,https://arxiv.org/pdf/2312.17653,,,,,,,,,LARP,197,,,,
AppAgent: Multimodal Agents as Smartphone Users,"Agent, GUI, MobileApp",2312,https://arxiv.org/pdf/2312.13771,,,,,,,,,AppAgent,178,,,,
CogAgent: A Visual Language Model for GUI Agents,"Agent, GUI",2312,https://arxiv.org/pdf/2312.08914,,,,,,,,,CogAgent,161,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQF5VhNYFxjBT3fzwDAwvxsYEYw6yUbJhUDFrO8RBKSfQ&s,,,
LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination,Agent,2312,https://arxiv.org/pdf/2312.15224,,,,,,,,,,160,,,,
Instruction-tuning Aligns LLMs to the Human Brain,"Brain, Instruction-Turning",2312,https://arxiv.org/pdf/2312.00575,,,,,,,,,,157,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTRO7ny6B0ZVTmaDx6AhdWwo5IHbmatnD4FNe7-1ZTDng&s,,,
Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis,"Robot, Survey",2312,https://arxiv.org/abs/2312.08782,,,https://www.connectedpapers.com/main/6140211405f9917ded519da50f00eee989eabd7f/Toward-General Purpose-Robots-via-Foundation-Models%3A-A-Survey-and-Meta Analysis/graph,,2023/12/14,,,Survey papers,Toward General-Purpose,38,,,,
Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases,"GPT4, Gemini, LLM",2312,https://arxiv.org/abs/2312.15011,,,,,2023/12/22,,,Quantitive Analysis,Gemini vs GPT-4V,44,,,,
"Foundation Models in Robotics: Applications, Challenges, and the Future","Foundation, Robot, Survey",2312,https://arxiv.org/abs/2312.07843,,,,,2023/12/13,,,Survey papers,Foundation Models,80,,,,
Language-conditioned Learning for Robotic Manipulation: A Survey,"Robot, Survey",2312,https://arxiv.org/abs/2312.10807,,,,,2023/12/17,,,Survey papers,Language-conditioned,94,,,,
LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding,"Perception, Robot",2312,https://arxiv.org/abs/2312.14074,,,,,2023/12/21,,,Multimodal Data injection,LiDAR-LLM,115,,,,
A Survey on Multimodal Large Language Models for Autonomous Driving,"Drive, Survey",2311,https://arxiv.org/abs/2311.12320,,,,,,,,,,432,,,,
Combating Misinformation in the Age of LLMs: Opportunities and Challenges,"Hallucination, Survey",2311,https://arxiv.org/abs/2311.05656,,,,,,,,,,385,,,,
GLaMM: Pixel Grounding Large Multimodal Model,"Google, Grounding",2311,https://arxiv.org/pdf/2311.03356,,,,,,,,,,349,,,,
Visual In-Context Prompting,"In-Context-Learning, Perception, Vision",2311,https://arxiv.org/pdf/2311.13601,,,,,,,,,,341,https://i.ytimg.com/vi/6_xjjbaKwjA/maxresdefault.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGBcgVCh_MA8=&rs=AOn4CLCLIKY7b56LPzhA1vl0k8RS1jgN4w,,,
LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models,"Image, LLaMA, Perception",2311,https://arxiv.org/pdf/2311.17043,,,,,,,,,,339,,,,
Vision-Language Instruction Tuning: A Review and Analysis,"Instruction-Turning, Survey",2311,https://arxiv.org/pdf/2311.08172,,,,,,,,,,296,,,,
Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld,"Agent, Embodied",2311,https://arxiv.org/abs/2311.16714,,,,,,,,,,291,,,,
Large Language Models for Robotics: A Survey,"LLM, Robot, Survey",2311,https://arxiv.org/abs/2311.07226,,,,,,,,,,289,,,,
Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning,"Reasoning, Symbolic",2311,https://arxiv.org/pdf/2311.17365,,,,,,,,,,284,,,,
Contrastive Chain-of-Thought Prompting,Reasoning,2311,https://arxiv.org/pdf/2311.09277,,,,,,,,,,247,,,,
"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions","Awesome Repo, Hallucination, Survey",2311,https://arxiv.org/abs/2311.05232,https://github.com/LuckyyySTA/Awesome-LLM-hallucination,,,,,,,,,242,https://s3.ap-northeast-1.amazonaws.com/wraptas-prod/layerx/1454988d-d891-4bb3-a38e-33aebb633385/a9c0f3addd469f4b26e263a34f4ad2e8.png,,,
Levels of AGI: Operationalizing Progress on the Path to AGI,"AGI, Survey",2311,https://arxiv.org/pdf/2311.02462,,,,,,,,,,199,,,,
Contrastive Chain-of-Thought Prompting,Prompting,2311,https://arxiv.org/pdf/2311.09277,,,,,,,,,,198,,,,
War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars,"Agent, Multi",2311,https://arxiv.org/abs/2311.17227,,,,,,,,,,185,,,,
Divergences between Language Models and Human Brains,"AGI, Brain",2311,https://arxiv.org/pdf/2311.09308,,,,,,70,,,,159,,,,
JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models,"Agent, Memory, Minecraft",2311,https://arxiv.org/abs/2311.05997,,,,,2023/11/10,,,Planning,JARVIS-1,27,,,,
Robot Learning in the Era of Foundation Models: A Survey,"Robot, Survey",2311,https://arxiv.org/abs/2311.14379,,,,,2023/11/24,,,Survey papers,Robot Learning,3,,,,
CogVLM: Visual Expert for Pretrained Language Models,"VLM, VQA",2311,https://arxiv.org/abs/2311.03079,,,,,2023/11/06,,,Visual Question Answering,CogVLM,55,,,,
GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?,"LLM, Zero-shot",2311,https://arxiv.org/abs/2311.15732,,,,,2023/11/27,,,Quantitive Analysis,GPT4Vis,18,,,,
The Development of LLMs for Embodied Navigation,"Embodied, LLM, Robot, Survey",2311,https://arxiv.org/abs/2311.00530,,,,,2023/11/01,,,Survey papers,The Development of LLMs,11,,,,
Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models,"Agent, Feedback, Reinforcement-Learning, Robot",2311,https://arxiv.org/abs/2311.02379,,,,,2023/11/04,,,Reinforcement Learning,Lafite-RL,20,,,,
RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation,"Data-generation, Robot",2311,https://arxiv.org/abs/2311.01455,,,,,2023/11/02,,,Data generation,RoboGen,79,,,,
Large Language Models as Generalizable Policies for Embodied Tasks,"Apple, Robot",2310,https://arxiv.org/pdf/2310.17722,,,,,,,,,,548,,,,
Ferret: Refer and Ground Anything Anywhere at Any Granularity,Apple,2310,https://arxiv.org/pdf/2310.07704,,,,,,,,,,542,,,,
Creative Robot Tool Use with Large Language Models,Robot,2310,https://arxiv.org/abs/2310.13065,,,,,,,,,,528,,,,
Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V,VisualPrompt,2310,https://arxiv.org/abs/2310.11441,https://som-gpt4v.github.io/,,,,,,,,,522,,,,
SoM : Set-of-Mark PromptingUnleashes Extraordinary Visual Grounding in GPT-4V,VisualPrompt,2310,https://arxiv.org/pdf/2310.11441,,,,,,,,,,521,,,,
Let's reward step by step: Step-Level reward model as the Navigators for Reasoning,PRM,2310,https://arxiv.org/abs/2310.10080,,,,,,,,,,495,,,,
Llemma: An Open Language Model For Mathematics,Math,2310,https://arxiv.org/pdf/2310.10631,,,,,,,,,,488,,,,
Mistral 7B,Open-source,2310,https://arxiv.org/abs/2310.06825,,,,,,,,,Mistral,480,,,,
AgentTuning: Enabling Generalized Agent Abilities For LLMs,"Agent, Instruction-Turning",2310,https://arxiv.org/abs/2310.12823,,,,,,,,Comprehensive Collection of LLM-Related Papers,,443,,,,
(Long)LLMLingua: Enhancing Large Language Model Inference via Prompt Compression,"Compress, Scaling",2310,https://arxiv.org/abs/2310.05736,,,,,,,,,,411,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQZolIr3Ll-gV-B4cchzSugQsnUmkMsA-e4Fp45a_jJzA&s,,,
AGENT INSTRUCTS LARGE LANGUAGE MODELS TO BE GENERAL ZERO-SHOT REASONERS,"Agent, Reasoning",2310,https://arxiv.org/pdf/2310.03710,,,,,,,,,,376,https://d3i71xaburhd42.cloudfront.net/e754e647ce86774040b1f05706e9809f18929a6a/2-Figure1-1.png,,,
"LEARNING EMBODIED VISION-LANGUAGE PRO- GRAMMING FROM INSTRUCTION, EXPLORATION, AND ENVIRONMENTAL FEEDBACK","Agent, Game",2310,https://arxiv.org/pdf/2310.08588,,,,,,,,,,372,,,,
Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,"Agent, Minecraft",2310,https://arxiv.org/pdf/2310.13255,,,,,,,,,,370,,,,
Creative Robot Tool Use with Large Language Models,"Code-as-Policies, Robot",2310,https://arxiv.org/pdf/2310.13065,,,,,,,,,,359,,,,
PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization,Agent,2310,https://arxiv.org/pdf/2310.16427,,,,,,,,,,352,,,,
SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding,"Apple, In-Context-Learning, Perception",2310,https://arxiv.org/pdf/2310.15308,,,,,,,,,,340,,,,
Tuna: Instruction Tuning using Feedback from Large Language Models,Instruction-Turning,2310,https://arxiv.org/abs/2310.13385,,,,,2023/03/06,,,,,334,,,,
Language Models as Zero-Shot Trajectory Generators,"LLM, PersonalCitation, Robot, Zero-shot",2310,https://arxiv.org/abs/2310.11604,,,,,,80,,,,254,"https://static.wixstatic.com/media/a40efa_2fa8ef4839fe4cacb8a1e85655103a90~mv2.png/v1/fill/w_945,h_481,al_c,q_90,usm_0.66_1.00_0.01,enc_auto/pipeline.png",,,
Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models,"Generation, Robot, Zero-shot",2310,https://arxiv.org/abs/2310.10639,,,,,,,,,,252,,,,
Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models,Reasoning,2310,https://arxiv.org/pdf/2310.06117,,,,,,,,,,250,,,,
FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"RAG, Temporal Logics",2310,https://arxiv.org/abs/2310.03214,,,,,,,,,,221,,,,
BitNet: Scaling 1-bit Transformers for Large Language Models,"LLM, Scaling",2310,https://arxiv.org/abs/2310.11453,,,,,,,,,,220,,,,
SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding,"Anything, CLIP, Perception",2310,https://arxiv.org/pdf/2310.15308,,,,,,,,,,201,,,,
AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents,"In-Context-Learning, Reinforcement-Learning",2310,https://arxiv.org/pdf/2310.09971,,,,,,,,,,153,,,,
OpenAgents: An Open Platform for Language Agents in the Wild,"Agent, Embodied",2310,https://arxiv.org/abs/2310.10634,https://github.com/xlang-ai/OpenAgents,,,,,,,,OpenAgents,150,,,,
DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,Agent,2310,https://arxiv.org/abs/2310.03714,,,,,,,,,DSPy,148,,,,
Large Language Models as Generalizable Policies for Embodied Tasks,"Embodied, Robot",2310,https://arxiv.org/pdf/2310.17722,,,,,,,,,,129,,,,
Eureka: Human-Level Reward Design via Coding Large Language Models,"Agent, Reinforcement-Learning",2310,https://arxiv.org/abs/2310.12931,,,,,2023/10/19,,,Reinforcement Learning,Eureka,63,,,,
Agent Instructs Large Language Models to be General Zero-Shot Reasoners,"Agent, Reasoning, Zero-shot",2310,https://arxiv.org/abs/2310.03710,,,,,2023/10/05,,,Planning,AgentInstruct,40,,,,
GPT-Driver: Learning to Drive with GPT,"Driving, Spacial",2310,https://arxiv.org/abs/2310.01415,,,,,2023/10/02,20,,Spatial Understanding,Gpt-driver,49,https://pointscoder.github.io/projects/gpt_driver/static/images/Arch5.png,,,
Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,"LLM, Spacial",2310,https://arxiv.org/abs/2310.03249,,,,,2023/10/05,,,Spatial Understanding,Path planners,57,,,,
GenSim: Generating Robotic Simulation Tasks via Large Language Models,"Data-generation, Robot",2310,https://arxiv.org/abs/2310.01361,,,,,2023/10/02,,,Data generation,Gensim,71,,,,
Qwen Technical Report,Open-source,2309,https://arxiv.org/pdf/2309.16609,,,,,,,,,,549,,,,
Guiding Instruction-based Image Editing via Multimodal Large Language Models,"Apple, VLM",2309,https://arxiv.org/pdf/2309.17102,,,,,,,,,,547,,,,
DynaCon: Dynamic Robot Planner with Contextual Awareness via LLMs,"Context-Awere, Context-Window",2309,https://arxiv.org/pdf/2309.16031,,,,,,,,,,498,,,,
RoFormer: Enhanced Transformer with Rotary Position Embedding,Context-Window,2309,https://arxiv.org/pdf/2309.09969,,,,,,,,,,449,https://miro.medium.com/v2/resize:fit:649/1*EzelwXA05g-1_MDCAjPHRg.png,,,
Prompt a Robot to Walk with Large Language Models,"Action-Generation, Generation, Prompting",2309,https://arxiv.org/pdf/2309.09969,,,,,,,,,,446,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSvYSFzjbtFCs-T81m8HNWgRN3k2bBFBj2KO8kb8Brcvg&s,,,
Cognitive Architectures for Language Agents,Agent,2309,https://arxiv.org/abs/2309.02427,,,,,,,,,40,426,,,,
You Only Look at Screens: Multimodal Chain-of-Action Agents,"Agent, MobileApp",2309,https://arxiv.org/pdf/2309.11436,https://github.com/cooelf/Auto-UI,,,,,,,,,287,,,,
CoALA: Awesome Language Agents,"Agent, Awesome Repo, LLM",2309,https://arxiv.org/pdf/2309.02427,https://github.com/ysymyth/awesome-language-agents,,,,,,,,,212,,,,
Agents: An Open-source Framework for Autonomous Language Agents,Agent,2309,https://arxiv.org/pdf/2309.07870,,,,,,,,,,202,,,,
MindAgent: Emergent Gaming Interaction,Agent,2309,https://arxiv.org/pdf/2309.09971,,,,,,,,,MindAgent,175,,,,
Agents: An Open-source Framework for Autonomous Language Agents,Agent,2309,https://arxiv.org/abs/2309.07870,https://github.com/aiwaves-cn/agents,,,,,,,,Agents,151,,,,
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.,Reasoning,2309,https://arxiv.org/pdf/2309.13007,,,,,,,,,ReConcile,132,,,,
"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","Chain-of-Thought, Reasoning, Survey",2309,https://arxiv.org/abs/2309.15402,,,,,2023/09/27,,,Survey Paper,A Survey of Chain of Thought Reasoning,1,,,,
Physically Grounded Vision-Language Models for Robotic Manipulation,"End2End, Multimodal, Robot",2309,https://arxiv.org/abs/2309.02561,,,,,2023/09/05,,,Multimodal LLM,Physically Grounded Vision-Language Model,32,,,,
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning,"Agent, Reinforcement-Learning, Reward",2309,https://arxiv.org/abs/2309.11489,,,,,2023/09/20,,,Reinforcement Learning,Text2Reward,66,,,,
AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback,"Agent, Feedback, Reinforcement-Learning",2309,https://arxiv.org/abs/2309.17176,,,,,2023/09/29,,,Reinforcement Learning,AdaRefiner,39,,,,
SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models,"Code-as-Policies, Robot",2309,https://arxiv.org/abs/2309.10062,,,,,2023/09/18,,,Code generation,SMART-LLM,56,,,,
The Rise and Potential of Large Language Model Based Agents: A Survey,"Agent, Survey",2309,https://arxiv.org/abs/2309.07864,,,,,2023/09/14,90,,Survey Paper,Large Language Model Based Agents,30,,,,
Prompt a Robot to Walk with Large Language Models,"Low-level-action, Robot",2309,https://arxiv.org/abs/2309.09969,,,,,2023/09/18,,,Low-level output,Prompt a Robot to Walk,31,,,,
LISA: Reasoning Segmentation via Large Language Model,Segmentation,2308,https://arxiv.org/pdf/2308.00692.pdf,,,,,,,,,,514,,,,
WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct,Math,2308,https://arxiv.org/pdf/2308.09583,,,,,,,,,,487,,,,
A Survey on Model Compression for Large Language Models,"Compress, Quantization, Survey",2308,https://arxiv.org/abs/2308.07633,,,,,,,,,,422,,,,
Learning to Model the World with Language,World-model,2308,https://arxiv.org/abs/2308.01399,,,,,,,,,,319,https://preview.redd.it/learning-to-model-the-world-with-language-uc-berkeley-2023-v0-8kluures85gb1.jpg?width=1358&format=pjpg&auto=webp&s=1f8b9a5c83f3e911ee170de7726cd2ee46262f74,,,
Learning to Model the World with Language,World-model,2308,https://arxiv.org/abs/2308.01399,,,,,,,,,,307,,,,
AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors,Agent,2308,https://arxiv.org/abs/2308.10848,,,,,,,,,AgentVerse,152,,,,
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,Agent,2308,https://arxiv.org/pdf/2308.08155,,,,,,,,,AutoGen,147,,,,
MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework,"Agent, Soft-Dev",2308,https://arxiv.org/pdf/2308.00352,,,,,,,,,MetaGPT,142,,,,
Code Llama: Open Foundation Models for Code,"Foundation, LLM, Open-source",2308,https://arxiv.org/pdf/2308.12950,,,,,,,,,Code LLaMA,131,,,,
AgentSims: An Open-Source Sandbox for Large Language Model Evaluation,Agent,2308,https://arxiv.org/abs/2308.04026,,,,,2023/08/08,,,Open-Source Evaluation,AgentSims,16,,,,
OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"Open-source, VLM",2308,https://arxiv.org/abs/2308.01390,,,,,2023/08/02,,,Vision-LLM,GPT4-V OpenFlamingo,33,,,,
Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models,"Chain-of-Thought, In-Context-Learning",2308,https://arxiv.org/abs/2308.10379,,,,,2023/08/20,,,Reasoning,Algorithm of Thoughts,34,,,,
A Survey on Large Language Model based Autonomous Agents,"Agent, Survey",2308,https://arxiv.org/abs/2308.11432,,,,,2023/08/22,,,Planning,Autonomous Agents,13,,,,
OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"LLM, Open-source",2308,https://arxiv.org/abs/2308.01390,,,,,2023/08/02,,,Open sourced LLM,OpenFlamingo,45,,,,
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning,"Chain-of-Thought, Planning, Reasoning",2308,https://arxiv.org/abs/2308.00436,,,,,2023/08/01,,,Planning,SelfCheck,2,,,,
MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation,"Multimodal, Robot",2308,https://arxiv.org/abs/2308.03624,,,,,2023/08/07,,,Multimodal prompts,MOMA-Force,5,,,,
ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,,2308,https://arxiv.org/abs/2308.07201,,,,,2023/08/14,,,Memory,ChatEval,102,,,,
Large Language Models as General Pattern Machines,Reasoning,2307,https://arxiv.org/abs/2307.04721,,,,,,,,,,535,,,,
Towards A Unified Agent with Foundation Models,"Reinforcement-Learning, Robot",2307,https://arxiv.org/abs/2307.09668,,,,,,,,,,527,,,,
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models,Robot,2307,https://arxiv.org/abs/2307.05973,,https://voxposer.github.io/,,,,,,,,470,,,,
ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs,"Agent, Tool",2307,https://arxiv.org/pdf/2307.16789,,,,,,,,,,377,,,,
Embodied Task Planning with Large Language Models,"Agent, Embodied",2307,https://arxiv.org/pdf/2307.01848,,,,,,,,,,374,,,,
Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation,"Chain-of-Thought, Reasoning",2307,https://arxiv.org/pdf/2307.15337,,,,,,,,,,367,,,,
"LONGNET: Scaling Transformers to 1,000,000,000 Tokens","Context-Window, Scaling",2307,https://arxiv.org/abs/2307.02486,,,,,2023/07/01,,,,,325,https://pbs.twimg.com/media/F0W92hXaAAQS0_k.jpg,,,
A Survey on Evaluation of Large Language Models,"Evaluation, LLM, Survey",2307,https://arxiv.org/abs/2307.03109,,,,,,,,,,245,,,,
RoCo: Dialectic Multi-Robot Collaboration with Large Language Models,Robot,2307,https://arxiv.org/abs/2307.04738,,,,25,,,,,RoCo,162,,,,
3D-LLM: Injecting the 3D World into Large Language Models,"3D, Open-source, Perception, Robot",2307,https://arxiv.org/abs/2307.12981,,,,,2023/07/24,,,Multimodal Data injection,3D-LLM,65,,,,
DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment,"Perception, Task-Decompose",2307,https://arxiv.org/abs/2307.00329,,,,,2023/07/01,,,Decomposing task,DOREMI,14,,,,
Embodied Task Planning with Large Language Models,"Embodied, Robot, Task-Decompose",2307,https://arxiv.org/abs/2307.01848,,,,,2023/07/04,,,Planning,Embodied Task Planning,19,,,,
SINC: Self-Supervised In-Context Learning for Vision-Language Tasks,"In-Context-Learning, VQA",2307,https://arxiv.org/abs/2307.07742,,,,,2023/07/15,,,Self-supervised,Self-supervised ICL,37,,,,
ARB: Advanced Reasoning Benchmark for Large Language Models,"Benchmark, In-Context-Learning",2307,https://arxiv.org/abs/2307.13692,,,,,2023/07/25,,,Benchmark,ARB,82,,,,
SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,"Robot, Task-Decompose",2307,https://arxiv.org/abs/2307.06135,,,,,2023/07/12,,,Decomposing task,SayPlan,87,,,,
Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding,"Chain-of-Thought, Reasoning",2307,https://arxiv.org/abs/2307.15337,,,,,2023/07/28,,,Chain of Thought,Skeleton-of-Thought,112,,,,
AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers,Robot,2306,https://arxiv.org/pdf/2306.06531,,,,,,,,,,534,,,,
"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","Agent, VLM",2306,https://arxiv.org/abs//2306.08640,,,,,,,,,,529,,,,
Textbooks Are All You Need,"SLM, Scaling",2306,https://arxiv.org/abs/2306.11644,,,,,,,,,,511,,,,
On the Design Fundamentals of Diffusion Models: A Survey,"Diffusion, Survey",2306,https://arxiv.org/abs/2306.04542,,,,,,,,,,434,,,,
"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",Agent,2306,https://arxiv.org/pdf/2306.08640,,,,,,,,,,331,,,,
Recognize Anything: A Strong Image Tagging Model,Perception,2306,https://arxiv.org/abs/2306.03514,,,,,,,,,,224,,,,
Large Language Models Are Semi-Parametric Reinforcement Learning Agents,Reinforcement-Learning,2306,https://arxiv.org/pdf/2306.07929,,,,,,,,,,156,,,,
Language to Rewards for Robotic Skill Synthesis,"Agent, Reinforcement-Learning",2306,https://arxiv.org/abs/2306.08647,,,,,2023/06/14,,,Reinforcement Learning,Language to Rewards,6,,,,
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,"Feedback, Robot",2306,https://arxiv.org/abs/2306.15724,,,,,2023/06/27,,,Self-improvement,REFLECT,12,,,,
Statler: State-Maintaining Language Models for Embodied Reasoning,"Code-as-Policies, PersonalCitation, Robot, State-Manage",2306,https://arxiv.org/abs/2306.17840,,,,,2023/06/30,,,Code generation,Statler,100,,,,
SayTap: Language to Quadrupedal Locomotion,"Low-level-action, Robot",2306,https://arxiv.org/abs/2306.07580,,,,,2023/06/13,,,Low-level output,SayTap,96,,,,
Let's Verify Step by Step,PRM,2305,https://arxiv.org/abs/2305.20050,,,,,,,,,,494,,,,
Gorilla: Large Language Model Connected with Massive APIs,"APIs, Agent, Tool",2305,https://arxiv.org/abs/2305.15334,,,,,,,,,,455,https://gorilla.cs.berkeley.edu/assets/img/gorilla_method.png,,,
Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory01,"Agent, Minecraft",2305,https://arxiv.org/pdf/2305.17144,,,,,,,,,,444,https://preview.redd.it/ghost-in-the-minecraft-generally-capable-agents-for-open-v0-7pbizoalma2b1.png?width=1861&format=png&auto=webp&s=1814afe1616b141dd974d895d96950b687abc8a9,,,
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,"LoRA, Scaling",2305,https://arxiv.org/abs/2305.03695,,,,,,,,,,429,,,,
Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought,"Chain-of-Thought, Code-as-Policies, PersonalCitation, Robot",2305,https://arxiv.org/abs/2305.16744,,,,,,,,,,310,,,,
Language Models Meet World Models,World-model,2305,https://arxiv.org/abs/2305.10626,,,,,,,,,,306,,,,
AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation,"Reasoning, Robot",2305,https://arxiv.org/pdf/2305.18898,,,,,,,,,,277,,,,
Gorilla: Large Language Model Connected with Massive APIs,"Agent, Tool",2305,https://arxiv.org/pdf/2305.15334,,,,,,,,,,276,,,,
EgoCOT: Embodied Chain-of-Thought Dataset for Vision Language Pre-training,"Chain-of-Thought, Embodied, Robot",2305,https://arxiv.org/pdf/2305.15021,,,,,,,,,,266,,,,
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes01,Distilling,2305,https://arxiv.org/pdf/2305.02301,,,,,,,,,,263,,,,
DetGPT: Detect What You Need via Reasoning,"Perception, Reasoning",2305,https://arxiv.org/abs/2305.14167,,,,,,,,,DetGPT,176,,,,
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,World-model,2305,https://arxiv.org/pdf/2305.14909,,,,,2023/05/07,,,,,172,https://www.catalyzex.com/_next/image?url=https%3A%2F%2Fd3i71xaburhd42.cloudfront.net%2F4f601b4e561557c7a0bd5a741a54cabaec7dc70e%2F4-Figure1-1.png&w=640&q=75,,,
NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models,"Navigation, Reasoning, Vision",2305,https://arxiv.org/pdf/2305.16986,,,,,,,,,NavGPT,171,,,,
When Brain-inspired AI Meets AGI,"AGI, Brain",2305,https://arxiv.org/pdf/2305.19352,,,,,,,,,,168,,,,
LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model,Brain,2305,https://arxiv.org/pdf/2305.19352,,,,,,,,,,158,,,,
Caption Anything: Interactive Image Description with Diverse Multimodal Controls,"Caption, VLM, VQA",2305,https://arxiv.org/abs/2305.02677,,,,,2023/05/04,,,Visual Question Answering,Caption Anything,7,,,,
InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language,"Intaractive, OpenGVLab, VLM",2305,https://arxiv.org/abs/2305.05662,,,,,2023/05/09,,,Vision-LLM,InternGPT,9,,,,
Small Models are Valuable Plug-ins for Large Language Models,In-Context-Learning,2305,https://arxiv.org/abs/2305.08848,,,,,2023/05/15,,,Reasoning,SuperICL,42,,,,
NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models,"LLM, Temporal Logics",2305,https://arxiv.org/abs/2305.07766,,,,,2023/05/12,,,Temporal Logics,NL2TL,50,,,,
Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,"Chain-of-Thought, In-Context-Learning",2305,https://arxiv.org/abs/2305.04091,,,,,2023/05/06,,,Reasoning,Plan-and-Solve,64,,,,
Voyager: An Open-Ended Embodied Agent with Large Language Models,"Agent, Minecraft",2305,https://arxiv.org/abs/2305.16291,,,,,2023/05/25,,,Planning,Voyager,62,,,,
Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement,"Chain-of-Thought, In-Context-Learning, Self",2305,https://arxiv.org/abs/2305.14497,,,,,2023/05/23,,,Reasoning,Self-Polish,70,,,,
ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst,"LLM, Open-source",2305,https://arxiv.org/abs/2305.16103,,,,,2023/05/25,,,Open sourced LLM,ChatBridge,73,,,,
Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,"Chain-of-Thought, Reasoning",2305,https://arxiv.org/abs/2305.03268,,,,,2023/05/05,,,Chain of Thought,Verify-and-Edit,26,,,,
Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Chain-of-Thought, Reasoning",2305,https://arxiv.org/abs/2305.10601,,,,,2023/05/17,,,Chain of Thought,Tree of Thought,61,,,,
Reasoning with Language Model is Planning with World Model,"Chain-of-Thought, In-Context-Learning",2305,https://arxiv.org/abs/2305.14992,,,,,2023/05/24,,,Reasoning,Reasoning via Planning,86,,,,
InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,"LLM, Open-source",2305,https://arxiv.org/abs/2305.06500,,,,,2023/05/11,,,Open sourced LLM,InstructBLIP,93,,,,
Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance,"Chain-of-Thought, Reasoning",2305,https://arxiv.org/abs/2305.17306,,,,,2023/05/26,,,Benchmark,Chain-of-Thought Hub,98,,,,
MemoryBank: Enhancing Large Language Models with Long-Term Memory,"LLM, Memory",2305,https://arxiv.org/abs/2305.10250,,,,,2023/05/17,,,Memory,MemoryBank,101,,,,
Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model,"Code-as-Policies, Multimodal, OpenGVLab, PersonalCitation, Robot",2305,https://arxiv.org/abs/2305.11176,,,,,2023/05/18,,,Multimodal prompts,Instruct2Act,105,https://miro.medium.com/v2/resize:fit:1096/1*EIUFVxb1Dri0EaYECy-JHg.png,,,
EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought,"Chain-of-Thought, Embodied, PersonalCitation, Robot, Task-Decompose",2305,https://arxiv.org/abs/2305.15021,,,,,2023/05/24,,,Chain of Thought,EmbodiedGPT,111,https://d3i71xaburhd42.cloudfront.net/00cb69a9f280317d1c59ac5827551ee9b10642b8/4-Figure2-1.png,,,
SegGPT: Segmenting Everything In Context,Segmentation,2304,https://arxiv.org/pdf/2304.03284,,,,,,,,,,524,,,,
SEEM: Segment Everything Everywhere All at Once,Segmentation,2304,https://arxiv.org/abs/2304.06718,https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once,,,,,,,,,519,,,,
What does CLIP know about a red circle? Visual prompt engineering for VLMs,VisualPrompt,2304,https://arxiv.org/abs/2304.06712,,,,,,,,,,482,,,,
What does CLIP know about a red circle? Visual prompt engineering for VLMs,In-Context-Learning,2304,https://arxiv.org/pdf/2304.06712,,,,,,,,,,342,https://www.catalyzex.com/_next/image?url=https%3A%2F%2Fd3i71xaburhd42.cloudfront.net%2Ff12ebcc9a0a7296cc6c85b243a003f7205c68b3d%2F1-Figure1-1.png&w=640&q=75,,,
INSTRUCTION TUNING WITH GPT-4,"GPT4, Instruction-Turning",2304,https://arxiv.org/abs/2304.03277,,,,,,,,,,299,,,,
Generative Agents: Interactive Simulacra of Human Behavior,Agent,2304,https://arxiv.org/abs/2304.03442,,,,,,,,,,126,,,,
Learning to Compress Prompts with Gist Tokens,"Compress, Prompting",2304,https://arxiv.org/abs/2304.08467,,,,,,,,,,125,,,,
Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models,"VLM, VQA",2304,https://arxiv.org/abs/2304.09842,,,,,2023/04/19,,,Visual Question Answering,Chameleon,29,,,,
Generative Agents: Interactive Simulacra of Human Behavior,In-Context-Learning,2304,https://arxiv.org/abs/2304.03442,,,,,2023/04/07,,,Memory,Generative Agents,68,,,,
MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,"Instruction-Turning, LLM",2304,https://arxiv.org/abs/2304.10592,,,,,2023/04/20,,,,MiniGPT-4,21,,,,
Segment Anything,"Anything, LLM, Open-source, Perception, Segmentation",2304,https://arxiv.org/abs/2304.02643,,,,,2023/04/05,,,Object Detection,Segment Anything,22,https://images.ctfassets.net/mauam998hl9x/L9hdF333CFCsBTSlYC7ey/b40a451118b366b1f4a5fccecb62971a/main.jpg,,,
Visual Instruction Tuning,"Instruction-Turning, LLM, PEFT",2304,https://arxiv.org/abs/2304.08485,,,,,2023/04/17,,,,LLaVA,43,,,,
LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,Agent,2304,https://arxiv.org/abs/2304.11477,,,,,2023/04/22,,,Planning,LLM+P,85,,,,
LLM as A Robotic Brain: Unifying Egocentric Memory and Control,"Memory, Robot",2304,https://arxiv.org/abs/2304.09349,,,,,2023/04/19,,,Brain,Robotic Brain,107,,,,
Language Instructed Reinforcement Learning for Human-AI Coordination,"Agent, Reinforcement-Learning",2304,https://arxiv.org/abs/2304.07297,,,,,2023/04/13,,,Reinforcement Learning,Language Instructed Reinforcement Learning,104,,,,
Sigmoid Loss for Language Image Pre-Training,Perception,2303,https://arxiv.org/abs/2303.15343,,,,,,,,,,536,,,,
Task and Motion Planning with Large Language Models for Object Rearrangement,"Robot, TAMP",2303,https://arxiv.org/abs/2303.06247,,,,,,,,,,533,,,,
Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers,MoE,2303,https://arxiv.org/abs/2303.01610,,,,,,,,,,486,,,,
Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,"Open-source, Perception",2303,https://arxiv.org/pdf/2303.05499,,,,,,,,,,204,,,,
CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society,Agent,2303,https://arxiv.org/pdf/2303.17760,,,,,,,,,CAMEL,146,,,,
Text2Motion: From Natural Language Instructions to Feasible Plans,"PersonalCitation, Robot",2303,https://arxiv.org/abs/2303.12153,,,,,,,,,,118,,,,
Open-World Object Manipulation using Pre-trained Vision-Language Models,"Multimodal, Robot, VLM",2303,https://arxiv.org/abs/2303.00905,,,,,2023/03/02,,,Multimodal LLM,MOO,41,,,,
Self-Refine: Iterative Refinement with Self-Feedback,"Chain-of-Thought, In-Context-Learning",2303,https://arxiv.org/abs/2303.17651,,,,,2023/03/30,,,Reasoning,Self-Refine,51,,,,
Reward Design with Language Models,"Agent, Reinforcement-Learning, Reward",2303,https://arxiv.org/abs/2303.00001,,,,,2023/02/27,,,Reinforcement Learning,Reward Design with Language Models,76,,,,
MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,"Reasoning, VLM, VQA",2303,https://arxiv.org/abs/2303.11381,,,,,2023/03/20,,,Visual Question Answering,MM-ReAct,24,,,,
Reflexion: Language Agents with Verbal Reinforcement Learning,Robot,2303,https://arxiv.org/abs/2303.11366,,,,,2023/03/20,,,Self-improvement,Reflexion,74,,,,
A Survey of Large Language Models,"LLM, Survey",2303,https://arxiv.org/abs/2303.18223,,,,,2023/03/31,,,Survey Papers,A Survey of Large Language Models,58,,,,
PaLM-E: An Embodied Multimodal Language Model,"End2End, Multimodal, Robot",2303,https://arxiv.org/abs/2303.03378,,,,,2023/03/06,,,Multimodal LLM,PaLM-E,54,,,,
ViperGPT: Visual Inference via Python Execution for Reasoning,"Code-as-Policies, Reasoning, VLM, VQA",2303,https://arxiv.org/abs/2303.08128,,,,,2023/03/14,,,Visual Question Answering,ViperGPT,4,,,,
Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,Perception,2303,https://arxiv.org/abs/2303.05499,,,,,2023/03/09,,,Object Detection,Grounding DINO,84,,,,
GPT-4 Technical Report,"GPT4, LLM",2303,https://arxiv.org/abs/2303.08774,,,,,2023/03/15,,,Closed sourced LLM,GPT4,91,,,,
ReAct: Synergizing Reasoning and Acting in Language Models,In-Context-Learning,2303,https://arxiv.org/abs/2303.11366,,,,,2023/03/20,,,Reasoning,ReAct,97,,,,
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,"Instruction-Turning, LLM, PEFT",2303,https://arxiv.org/abs/2303.16199,,,,,2023/03/28,,,,LLaMA-adapter,108,,,,
Exploring the Benefits of Training Expert Language Models over Instruction Tuning,Instruction-Turning,2302,https://arxiv.org/abs/2302.03202,,,,27,2023/02/06,,,,,332,,,,
Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,"Grounding, Reinforcement-Learning",2302,https://arxiv.org/pdf/2302.02662,,,,,,,,,,187,,,,
Guiding Pretraining in Reinforcement Learning with Large Language Models,"Agent, Reinforcement-Learning",2302,https://arxiv.org/abs/2302.06692,,,,,2023/02/13,,,Reinforcement Learning,ELLM,8,,,,
"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","Agent, Minecraft",2302,https://arxiv.org/abs/2302.01560,,,,,2023/02/03,,,Planning,DEPS,47,,,,
LLaMA: Open and Efficient Foundation Language Models,"Foundation, LLM, Open-source",2302,https://arxiv.org/abs/2302.13971,,,,,2023/02/27,80,,Open sourced LLM,LLaMA,92,,,,
Multimodal Chain-of-Thought Reasoning in Language Models,"Chain-of-Thought, Reasoning",2302,https://arxiv.org/abs/2302.00923,,,,,2023/02/02,,,Chain of Thought,Multimodal-CoT,116,,,,
A Survey on In-context Learning,"In-Context-Learning, Survey",2301,https://arxiv.org/abs/2301.00234,,,,,,,,,,384,,,,
Rethinking with Retrieval: Faithful Large Language Model Inference,"Chain-of-Thought, Reasoning",2301,https://arxiv.org/abs/2301.00303,,,,,2022/12/31,,,Chain of Thought,Rethinking with Retrieval,110,,,,
Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Instruction-Turning, Self",2212,https://arxiv.org/pdf/2212.10560,,,,817,,,,,,382,,,https://www.semanticscholar.org/paper/Self-Instruct%3A-Aligning-Language-Models-with-Wang-Kordi/e65b346d442e9962a4276dc1c1af2956d9d5f1eb,
Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,"In-Context-Learning, Scaling",2212,https://arxiv.org/pdf/2212.09095,,,,18,2022/03/06,,,,,336,,,,
"Structured Prompting: Scaling In-Context Learning to 1,000 Examples","In-Context-Learning, Scaling",2212,https://arxiv.org/pdf/2212.06713,,,,20,2020/03/06,,,,,335,,,,
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"Agent, LLM, Planning",2212,https://arxiv.org/pdf/2212.04088,,,,,,,,,,227,,,,
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"Agent, Embodied",2212,https://arxiv.org/pdf/2212.04088,,,,,,,,,,174,,,,
Reasoning with Language Model Prompting: A Survey,"Reasoning, Survey",2212,https://arxiv.org/abs/2212.09597,,,,,,,,,,124,,,,
Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,"Chain-of-Thought, Reasoning, Survey",2212,https://arxiv.org/abs/2212.10001,,,,,2023/12/20,,,,,120,,,,
Towards Reasoning in Large Language Models: A Survey,"LLM, Reasoning, Survey",2212,https://arxiv.org/abs/2212.10403,,,,,2022/12/20,,,Survey Paper,Reasoning in Large Language Models,48,,,,
Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Instruction-Turning, LLM",2212,https://arxiv.org/abs/2212.10560,,,,,2022/12/20,,,,Self-Instruct,103,,,,
PAL: Program-aided Language Models,"Chain-of-Thought, In-Context-Learning",2211,https://arxiv.org/abs/2211.10435,,,,,2022/11/18,,,Reasoning,PAL,52,,,,
Visual Programming: Compositional visual reasoning without training,"Code-as-Policies, VLM, VQA",2211,https://arxiv.org/abs/2211.11559,,,,,2022/11/18,,,Visual Question Answering,VISPROG,99,,,,
Large Language Models Are Human-Level Prompt Engineers,"Automate, Prompting",2211,https://arxiv.org/abs/2211.01910,,,,,2022/11/03,,,Automation,APE,114,,,,
Interactive Language: Talking to Robots in Real Time,Robot,2210,https://arxiv.org/pdf/2210.06407,,,,,,,,,,130,,,,
Large Language Models are few(1)-shot Table Reasoners,"Reasoning, Table",2210,https://arxiv.org/abs/2210.06710,,,,,,,,,,123,,,,
Automatic Chain of Thought Prompting in Large Language Models,"Automate, Chain-of-Thought, Reasoning",2210,https://arxiv.org/abs/2210.03493,,,,,2022/10/07,,,Chain of Thought,Auto-CoT,53,,,,
Complexity-Based Prompting for Multi-Step Reasoning,"Chain-of-Thought, In-Context-Learning",2210,https://arxiv.org/abs/2210.00720,,,,,2022/10/03,,,Reasoning,COMPLEXITY-CoT,35,,,,
VIMA: General Robot Manipulation with Multimodal Prompts,"End2End, Multimodal, Robot",2210,https://arxiv.org/abs/2210.03094,,,,,2022/10/06,,,Multimodal prompts,VIMA,59,,,,
Measuring and Narrowing the Compositionality Gap in Language Models,"Chain-of-Thought, In-Context-Learning, Self",2210,https://arxiv.org/abs/2210.03350,,,,,2022/10/07,,,Reasoning,Self-Ask,36,,,,
Visual Prompting via Image Inpainting,"In-Context-Learning, Vision",2209,https://arxiv.org/abs/2209.00647,,,,,,,,,,344,,,,
Code as Policies: Language Model Programs for Embodied Control,"Code-as-Policies, Embodied, PersonalCitation, Robot",2209,https://arxiv.org/abs/2209.07753,,,,,2022/09/16,,,Code generation,Code as policies,10,https://code-as-policies.github.io/img/share_image.png,,,
ProgPrompt: Generating Situated Robot Task Plans using Large Language Models,"Code-as-Policies, PersonalCitation, Robot",2209,https://arxiv.org/abs/2209.11302,,,,,2022/09/22,,,Code generation,Progprompt,95,,,,
RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents,Reinforcement-Learning,2208,https://arxiv.org/pdf/2208.06448,,,,,,,,,RLang,155,,,,
Inner Monologue: Embodied Reasoning through Planning with Language Models,"Code-as-Policies, Embodied, PersonalCitation, Reasoning, Robot, Task-Decompose",2207,https://arxiv.org/abs/2207.05608,,,,,,,,,,119,https://innermonologue.github.io/img/teaser.png,,,
Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,In-Context-Learning,2206,https://arxiv.org/abs/2206.04615,,,,,2022/06/09,,,Benchmark,BIG-Bench,28,,,,
EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL,"Agent, Reinforcement-Learning, Reward",2206,https://arxiv.org/abs/2206.09674,,,,,2022/06/20,,,Reinforcement Learning,EAGER,78,,,,
PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Benchmark, In-Context-Learning",2206,https://arxiv.org/abs/2206.10498,,,,,2022/06/21,,,Benchmark,PlanBench,88,,,,
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,"Computer-Resource, Scaling",2205,https://arxiv.org/pdf/2205.14135.pdf,,,,,,,,,,506,,,,
Simple Open-Vocabulary Object Detection with Vision Transformers,Perception,2205,https://arxiv.org/abs/2205.06230,,,,,,,,,,313,,,,
Large Language Models are Zero-Shot Reasoners,"Reasoning, Zero-shot",2205,https://arxiv.org/pdf/2205.11916,,,,,,,,,,257,https://i0.wp.com/syncedreview.com/wp-content/uploads/2022/05/image-90.png?resize=950%2C501&ssl=1,,,
Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,Reasoning,2205,https://arxiv.org/abs/2205.09712,,,,,,,,,Selection-Inference,140,,,,
Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations,"Chain-of-Thought, In-Context-Learning",2205,https://arxiv.org/abs/2205.11822,,,,,2022/05/24,,,Reasoning,Maieutic Prompting,60,,,,
Simple Open-Vocabulary Object Detection with Vision Transformers,Perception,2205,https://arxiv.org/abs/2205.06230,,,,,2022/05/12,,,Object Detection,OWL-ViT,17,,,,
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,"Chain-of-Thought, In-Context-Learning",2205,https://arxiv.org/abs/2205.10625,,,,,2022/05/21,,,Reasoning,Least-to-Most Prompting,25,,,,
A Generalist Agent,"Agent, Multimodal, Robot",2205,https://arxiv.org/abs/2205.06175,,,,,2022/05/12,,,Multimodal LLM,GATO,81,,,,
Correcting Robot Plans with Natural Language Feedback,"Feedback, Robot",2204,https://arxiv.org/abs/2204.05186,,,,,,,,,,356,,,,
Flamingo: a Visual Language Model for Few-Shot Learning,"Multimodal, Robot",2204,https://arxiv.org/abs/2204.14198,,,,,2022/04/29,,,Multimodal LLM,Flamingo,67,,,,
PaLM: Scaling Language Modeling with Pathways,VLM,2204,https://arxiv.org/abs/2204.02311,,,,,2022/04/05,,,Vision-LLM,PaLM,46,,,,
Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,"Code-as-Policies, PersonalCitation, Robot, Zero-shot",2204,https://arxiv.org/abs/2204.00598,,,,,2022/04/01,,,Code generation,Socratic,90,,,,
"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","LLM, Robot, Task-Decompose",2204,https://arxiv.org/abs/2204.01691,,,,,2022/04/04,,,Decomposing task,SayCan,117,,,,
Visual Prompt Tuning,"In-Context-Learning, Prompt-Tuning",2203,https://arxiv.org/pdf/2203.12119,,,,,,,,,,346,https://raw.githubusercontent.com/KMnP/vpt/main/imgs/teaser.png,,,
STaR: Bootstrapping Reasoning With Reasoning,Reasoning,2203,https://arxiv.org/abs/2203.14465,,,,,2022/05/28,,,,,327,,,,
DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection,Perception,2203,https://arxiv.org/pdf/2203.03605,,,,,,,,,,205,,,,
Training language models to follow instructions with human feedback,"Instruction-Turning, LLM",2203,https://arxiv.org/abs/2203.02155,,,,,2022/03/04,,,,InstructGPT,109,,,,
Self-Consistency Improves Chain of Thought Reasoning in Language Models,"Chain-of-Thought, Reasoning",2203,https://arxiv.org/abs/2203.11171,,,,,2022/03/21,,,Reasoning,Self-Consistency,113,,,,
BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning,"Robot, Zero-shot",2202,https://arxiv.org/abs/2202.02005,,,,,,,,,BC-Z,214,https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQOGz8vcNIUjIZmwpap0f2epSp-wM7IMuRoj1Huk-2_&s,,,
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"Chain-of-Thought, Reasoning",2201,https://arxiv.org/abs/2201.11903,,,,,2022/01/28,50,,Chain of Thought,Chain of Thought,15,,,,
Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,"Robot, Task-Decompose, Zero-shot",2201,https://arxiv.org/abs/2201.07207,,,,,2022/01/18,,,Decomposing task,Language Models as Zero-Shot Planners,83,,,,
PointCLIP: Point Cloud Understanding by CLIP,Perception,2112,https://arxiv.org/abs/2112.02413,,,,,2021/12/04,,,Object Detection,PointCLIP,72,,,,
Grounded Language-Image Pre-training,Perception,2112,https://arxiv.org/abs/2112.03857,,,,,2021/12/07,,,Object Detection,GLIP,106,,,,
Visually Grounded Reasoning across Languages and Cultures,"Grounding, Reasoning",2109,https://arxiv.org/pdf/2109.13238,,,,,,,,,,347,,,,
Finetuned Language Models Are Zero-Shot Learners,"Instruction-Turning, LLM, Zero-shot",2109,https://arxiv.org/abs/2109.01652,,,,,2021/09/03,,,,FLAN,77,,,,
LoRA: Low-Rank Adaptation of Large Language Models,"LoRA, Scaling",2106,https://arxiv.org/pdf/2106.09685,,,,,,,,,,418,,,,
CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval,"Perception, Video, Vision",2104,https://arxiv.org/abs/2104.08860,,,,,,,,,,338,,,,
RoFormer: Enhanced Transformer with Rotary Position Embedding,RoPE,2104,https://arxiv.org/abs/2104.09864,,,,,,,,,,272,,,,
Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,MoE,2101,https://arxiv.org/abs/2101.03961,,,,,,,,,,305,,,,
LLM Powered Autonomous Agents,"Agent, Blog",2023,https://lilianweng.github.io/posts/2023-06-23-agent/,,,,,,,,,,378,,,,
Language Models are Few-Shot Learners,LLM,2005,https://arxiv.org/abs/2005.14165,,,,,2020/05/28,,,Closed sourced LLM,GPT3,89,,,,
VisualCOMET: Reasoning about the Dynamic Context of a Still Image,"In-Context-Learning, VQA",2004,https://arxiv.org/abs/2004.10796,,,,,2020/04/22,,,Reasoning,VisualCOMET,69,,,,
"Generalized Decoding for Pixel, Image, and Language",Segmentation,,https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf,,,,,,,,,,518,,,,
GRES: Generalized Referring Expression Segmentation,Segmentation,,,,,,,,,,,,517,,,,
MoE-LLaVA: Mixture of Experts for Large Vision-Language Models,"LLaVA, MoE, VLM",,,,,,,,,,,,510,,,,
Instructor: Structured LLM Outputs,Package,,,https://github.com/jxnl/instructor,,,,,,,,,505,,,,
unsloth,Package,,,https://github.com/unslothai/unsloth,,,,,,,,,504,,,,
LMSYS Chatbot Arena Leaderboard,"LLM, Leaderboard",,,,,,,,,,,,503,,,,
Mantis: Multi-Image Instruction Tuning,"Multi-Images, VLM",,https://tiger-ai-lab.github.io/Blog/mantis,https://tiger-ai-lab.github.io/Blog/mantis,,,,,,,,,502,,,,
"LLaVA-NeXT: Improved reasoning, OCR, and world knowledge",VLM,,,https://llava-vl.github.io/blog/2024-01-30-llava-next/,,,,,,,,,501,,,,
awesome-vlm-architectures,"Awesome Repo, VLM",,,https://github.com/gokayfem/awesome-vlm-architectures,,,,,,,,,499,,,,
PRM800K: A Process Supervision Dataset,Datatset,,,https://github.com/openai/prm800k,,,,,,,,,496,,,,
Awesome Scientific Language Models,"Awesome Repo, Math, Science",,,https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models,,,,,,,,,492,,,,
Promptlayer,Package,,,https://promptlayer.com/,,,,,,,,,491,,,,
Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study,Agent,,,,,,,,,,,,490,,,,
LET’S REWARD STEP BY STEP: STEP-LEVEL REWARD MODEL AS THE NAVIGATORS FOR REASONING,"Reasoning, Reward",,https://openreview.net/pdf?id=RSQL6xvUYW,,,,,,,,,,489,,,,
,Evaluation,,,,,,,,,,,,485,,,,
simple-evals,Evaluation,,,https://github.com/openai/simple-evals,,,,,,,,,484,,,,
Penn State University,Lab,,,,,,,,,,,,474,,,,
Fudan NLP Group,Lab,,,,,,,,,,,,473,,,,
tsinghua,Lab,,,,,,,,,,,,472,,,,
sensetime,Lab,,,,,,,,,,,,471,,,,
open-interpreter,"Agent-Project, Code-LLM",,,https://github.com/OpenInterpreter/open-interpreter,,,,,,,,,469,,,,
awesome-korean-llm,"Awesome Repo, Korean",,,https://github.com/NomaDamas/awesome-korean-llm,,,,,,,,,465,,,,
Awesome-Chinese-LLM,"Awesome Repo, Chinese",,,https://github.com/HqWu-HITCS/Awesome-Chinese-LLM,,,,,,,,,464,,,,
LaVague,"Action-Model, Agent, LAM",,,https://github.com/lavague-ai/LaVague,,,,,,,,,462,,,,
"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",RAG,,,,,,,,,,,,453,,,,
ScreenAI: A Vision-Language Model for UI and Infographics Understanding,VLM,,,,,,,,,,,,445,,,,
Paper List for In-context Learning,"Awesome Repo, In-Context-Learning",,,https://github.com/dqxiu/ICL_PaperList,,,,,,,,,440,,,,
IROS2023PaperList,"Awesome Repo, IROS, Robot",,,https://github.com/gonultasbu/IROS2023PaperList,,,,,,,,,439,,,,
GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration,"Demonstration, GPT4, PersonalCitation, Robot, VLM",,,,,,,,,,,,438,,,,
Tree-Planner: Efficient Close-loop Task Planning with Large Language Models01,"LLM, PersonalCitation, Robot",,,,,,,,,,,,437,,,,
DeepSeek-VL: Towards Real-World Vision-Language Understanding01,"VLM, VQA",,,,,,,,,,,,436,,,,
Awesome LLM-Powered Agent,"Agent, Awesome Repo",,,https://github.com/hyp1231/awesome-llm-powered-agent,,,,,,,,,428,,,,
LLM Agents Papers,"Agent, Awesome Repo",,,https://github.com/zjunlp/LLMAgentPapers,,,,,,,,,427,,,,
Awesome LLM Compression,"Awesome Repo, Compress",,,https://github.com/HuangOwen/Awesome-LLM-Compression,,,,,30,,Awesome LLM Compression,,424,,,,
A latent text-to-image diffusion model,Diffusion,,,,,,,,,,,,421,,,,
Robust Speech Recognition via Large-Scale Weak Supervision,Audio,,,,,,,,,,,,420,,,,
Alpaca-LoRA,Package,,,https://github.com/tloen/alpaca-lora,,,,,,,,,417,,,,
Dify,Package,,,https://github.com/langgenius/dify,,,,,,,,,416,,,,
Awesome LLMOps,"Awesome Repo, Package",,,https://github.com/tensorchord/Awesome-LLMOps,,,,,60,,,,415,,,,
h2oGPT,Package,,,https://github.com/h2oai/h2ogpt,,,,,,,,,414,,,,
LangChain,Package,,,https://github.com/langchain-ai/langchain,,,,,,,,,413,,,,
LlamaIndex,Package,,,https://github.com/run-llama/llama_index,,,,,,,,,412,,,,
A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization,Brain,,,,,,,,,,,,410,,,,
Could a Large Language Model be Conscious?,"Brain, Conscious",,,,,,,,,,,,408,,,,
[Resource] arxiv-sanity,Resource,,https://arxiv-sanity-lite.com/,,,,,,,,,,400,,,,
[Resource] AlphaSignal,Resource,,https://alphasignal.ai/,,,,,,,,,,399,,,,
Imperial College London - Zeroshot trajectory,Lab,,,,,,,,,,,,397,,,,
OpenGVLab,Lab,,,https://github.com/OpenGVLab,,,,,,,,,396,,,,
Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook,"Survey, TimeSeries",,,,,,,,,,,,395,https://d3i71xaburhd42.cloudfront.net/5b038c1a93967072cc76689fd805e756f804cc42/2-Figure1-1.png,,,
Awesome Vision-Language Navigation,"Awesome Repo, Perception, VLM",,,https://github.com/daqingliu/awesome-vln,,,,,,,,,394,,,,
Awesome Large Multimodal Agents,"Agent, Awesome Repo",,,https://github.com/jun0wanan/awesome-large-multimodal-agents,,,,,70,,,,393,,,,
Awesome-LLM-Survey,"Awesome Repo, LLM, Survey",,,https://github.com/HqWu-HITCS/Awesome-LLM-Survey,,,,,100,,,,392,,,,
A Survey on Knowledge Distillation of Large Language Models,"Distilling, Survey",,,,,,,,,,,,390,,,,
Awesome-Papers-Autonomous-Agent,"Agent, Awesome Repo",,,https://gh.mlsub.net/lafmdp/Awesome-Papers-Autonomous-Agent,,,,,,,,,386,,,,
[Resource] Semanticscholar,Resource,,https://www.semanticscholar.org,,,,,,,,,,383,,,,
ScreenAgent: A Computer Control Agent Driven by Visual Language Large Model,"Agent, GUI",,,https://github.com/niuzaisheng/ScreenAgent,,,,,,,,,379,,,,
You Only Look at Screens: Multimodal Chain-of-Action Agents,"Agent, GUI, MobileApp",,,,,,,,,,,,371,,,,
Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University -  CogVLM,Lab,,,,,,,,,,,,369,,,,
"Rutgers University, AGI Research - OpenAGI",Lab,,,,,,,,,,,,368,,,,
XLANG NLP Lab - OpenAgents,Lab,,,,,,,,,,,,366,,,,
"OpenBMB - ChatDev, XAgent, AgentVerse",Lab,,,,,,,,,,,,365,,,,
Reworkd AI - AgentGPT,Lab,,,,,,,,,,,,364,,,,
DeepWisdom - MetaGPT,Lab,,,,,,,,,,,,363,,,,
"Tencent AI Lab - AppAgent, WebVoyager",Lab,,,,,,,,,,,,362,,,,
ChatGPT for Robotics: Design Principles and Model Abilities,"Code-as-Policies, PersonalCitation, Robot",,,,,,,,,,,,361,,,,
RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks,"Code-as-Policies, PersonalCitation, Robot",,,,,,,,,,,,360,,,,
Autonomous Agents,"Agent, Awesome Repo",,,https://github.com/tmgthb/Autonomous-Agents,,,,,,,,,353,,,,
Awesome Embodied Vision,"Awesome Repo, Embodied",,,https://github.com/ChanganVR/awesome-embodied-vision,,,,,,,,,351,,,,
V-IRL: Grounding Virtual Intelligence in Real Life,Grounding,,,,,,,,,,,,348,,,,
Prompting Visual-Language Models for Efficient Video Understanding,"In-Context-Learning, Video",,,,,,,,,,,,345,,,,
What Makes Good Examples for Visual In-Context Learning?,"In-Context-Learning, Vision",,,,,,,,,,,,343,,,,
In-Context Instruction Learning,"In-Context-Learning, Instruction-Turning",,,,,,,,,,,,333,https://pbs.twimg.com/media/FqGlB2lWwAAPyX3.jpg,,,
OpenAGI: When LLM Meets Domain Experts,"AGI, Agent",,,,,,54,,,,,,330,,,,
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,"Context-Window, Foundation, Gemini, LLM, Scaling",,,,,,,,,,,,324,,,,
XLang Paper Reading,"Agent, Awesome Repo, Embodied, Grounding",,,https://github.com/xlang-ai/xlang-paper-reading,,,,,70,,,,316,,,,
Application of Pretrained Large Language Models in Embodied Artificial Intelligence,"Agent, Embodied, Survey",,https://www.semanticscholar.org/paper/Application-of-Pretrained-Large-Language-Models-in-Kovalev-Panov/04f87baf7d1b3eb303a52a8a66c8189f396dd114,,,,,,,,,,312,,,,
Training Language Models with Memory Augmentation,RAG,,,,,,,,,,,,304,,,,
[Resource] Connectedpapers,Resource,,https://www.connectedpapers.com/,,,,,,,,,,302,,,,
Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models,"Datatset, Instruction-Turning",,,,,,,,,,,,301,,,,
Exploring Format Consistency for Instruction Tuning,Instruction-Turning,,,,,,,,,,,,300,,,,
REVO-LION: EVALUATING AND REFINING VISION LANGUAGE INSTRUCTION TUNING DATASETS,"Datatset, Instruction-Turning",,,,,,,,,,,,298,,,,
A Closer Look at the Limitations of Instruction Tuning,Instruction-Turning,,,,,,,,,,,,297,,,,
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning,"Instruction-Turning, Survey",,,,,,,,,,,,295,,,,
Octopus: Embodied Vision-Language Programmer from Environmental Feedback,"Agent, Embodied",,,,,,,,,,,,292,,,,
[Resource] dailyarxiv,Resource,,https://dailyarxiv.com/,,,,,,,,,,285,,,,
Instruction Tuning for Large Language Models: A Survey,"Instruction-Turning, LLM, Survey",,,,,,,,,,,,281,,,,
A Survey of Reinforcement Learning from Human Feedback,"RLHF, Reinforcement-Learning, Survey",,,,,,,,,,,,280,,,,
Multimodal & Large Language Models,"Awesome Repo, LLM, VLM",,,https://github.com/Yangyi-Chen/Multimodal-AND-Large-Language-Models,,,,,90,,,,279,,,,
Introspective Tips: Large Language Model for In-Context Decision Making,Robot,,https://www.semanticscholar.org/paper/Introspective-Tips%3A-Large-Language-Model-for-Making-Chen-Wang/047e3812854a86b2a2e113219fa956eda860ce24,,,,,,,,,,278,,,,
[Resource] huggingface,Resource,,https://huggingface.co/papers,,,,,,,,,,275,,,,
[Resource] Paperswithcode,Resource,,https://paperswithcode.com/,,,,,,,,,,274,,,,
Language Models Meet World Models: Embodied Experiences Enhance Language Models,"Embodied, World-model",,,,,,,,,,,,273,,,,
RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation,Robot,,,,,,,,,,,,271,,,,
Learning and Leveraging World Models in Visual Representation Learning,World-model,,,,,,,,,,,,269,,,,
Language Segment-Anything,"Perception, Robot, Segmentation",,,,,,,,,,,,267,,,,
LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models01,"Benchmark, Sora, Text-to-Video",,,,,,,,,,,,265,,,,
TaskWeaver: A Code-First Agent Framework,"Agent, Code-LLM",,,,,,,,,,,,264,,,,
MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT,"LLaMA, Lightweight, Open-source",,,,,,,,,,,,262,,,,
"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models","Sora, Text-to-Video",,,,,,,,,,,,261,,,,
StarCoder 2 and The Stack v2: The Next Generation,Code-LLM,,,,,,,,,,,,260,,,,
Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?,Zero-shot,,,,,,,,,,,,258,,,,
Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning,"Prompting, Robot, Zero-shot",,https://arxiv.org/pdf/1706.05064,,,,,,,,,,256,,,,
Towards Generalizable Zero-Shot Manipulationvia Translating Human Interaction Plans,"Generation, Robot, Zero-shot",,,,,,,,,,,,253,,,,
Rephrase and Respond(RaR),Reasoning,,,,,,,,,,,,249,,,,
"Retrieval-Augmented Generation for Large Language ","RAG, Survey",,,,,,,,,,,,248,,,,
Advances in 3D Generation: A Survey,"Generation, Survey",,,,,,,,,,,,244,,,,
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens,"Context-Window, Scaling",,,,,,,,,,,,243,,,,
ReFT: Reasoning with Reinforced Fine-Tuning,"Reasoning, Reinforcement-Learning",,,,,,,,,,,,237,,,,
MM-LLMs: Recent Advances in MultiModal Large Language Models,"Survey, VLM",,,,,,,,,,,,235,,,,
Awesome-Diffusion-Models,"Awesome Repo, Diffusion",,,https://github.com/diff-usion/Awesome-Diffusion-Models,,,,,,,,,232,,,,
Gemma: Introducing new state-of-the-art open models,Open-source,,https://blog.google/technology/developers/gemma-open-models/,,,,,,,,,,230,,,,
swarms,Agent,,,https://github.com/kyegomez/swarms,,,,,,,,,229,,,,
Video as the New Language for Real-World Decision Making,"Agent, Video-for-Agent",,,,,,,,,,,,226,,,,
Large Language Models for Information Retrieval: A Survey,"RAG, Survey",,,,,,,,,,,,222,,,,
Awesome-LLM-Papers-Toward-AGI,"AGI, Awesome Repo, Survey",,,https://github.com/shure-dev/Awesome-LLM-Papers-Toward-AGI,,,,,,,,,218,,,,
Awesome AI Agents,"Agent, Awesome Repo",,,https://github.com/e2b-dev/awesome-ai-agents,,,,,,,,,213,,,,
LLM-in-Vision,"Awesome Repo, LLM, Vision",,,https://github.com/DirtyHarryLYL/LLM-in-Vision,,,,,,,,,211,,,,
Awesome RLHF (RL with Human Feedback),"Awesome Repo, RLHF, Reinforcement-Learning",,,https://github.com/opendilab/awesome-RLHF,,,,,,,,,210,,,,
Chain-of-ThoughtsPapers,"Awesome Repo, Chain-of-Thought",,,https://github.com/Timothyxxx/Chain-of-ThoughtsPapers,,,,,,,,,209,,,,
LLM-Leaderboard,"Awesome Repo, LLM, Leaderboard",,,https://github.com/LudwigStumpp/llm-leaderboard,,,,,,,,,208,,,,
日本語LLMまとめ,"Awesome Repo, Japanese, LLM",,,https://github.com/llm-jp/awesome-japanese-llm,,,,,,,,,207,,,,
A Survey on LLM-based Autonomous Agents,"Agent, Robot, Survey",,,https://github.com/Paitesanshi/LLM-Agent-Survey,,,,,,,,,200,https://d3i71xaburhd42.cloudfront.net/28c6ac721f54544162865f41c5692e70d61bccab/4-Figure2-1.png,,,
Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception,"Agent, GUI, MobileApp",,,,,,,,,,,Mobile-Agent,191,https://arxiv.org/html/2401.16158v1/extracted/5373117/first_image.jpg,,,
Awesome-LLM,"Awesome Repo, LLM",,,https://github.com/Hannibal046/Awesome-LLM,,,,,,,,,189,,,,
Awesome-Reasoning-Foundation-Models,"Awesome Repo, Reasoning",,,https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models,,,,,,,,,188,,,,
Everything-LLMs-And-Robotics,"Awesome Repo, LLM, Robot",,,https://github.com/jrin771/Everything-LLMs-And-Robotics,,,,,50,,,,186,,,,
Awesome-Multimodal-LLM,"Awesome Repo, Multimodal",,,https://github.com/Atomic-man007/Awesome_Multimodel_LLM,,,,,,,,,184,,,,
LLMSurvey,"Awesome Repo, Survey",,,https://github.com/RUCAIBox/LLMSurvey,,,,,,,,,183,,,,
Awesome LLM Reasoning,"Awesome Repo, Reasoning",,,https://github.com/atfortes/Awesome-LLM-Reasoning,,,,,,,,,182,,,,
Awesome-LLM-Robotics,"Awesome Repo, Robot",,,https://github.com/GT-RIPL/Awesome-LLM-Robotics,,,,,,,,,181,,,,
Awesome-Embodied-Agent-with-LLMs,"Agent, Awesome Repo, LLM",,,https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs,,,,,90,,,,180,,,,
Awesome-Multimodal-Large-Language-Models,"Awesome Repo, Multimodal",,,https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models,,,,,,,,,179,,,,
Embodied Question Answering,Enbodied,,https://arxiv.org/abs/1711.11543,,,,,,,,,,177,,,,
STARLING: SELF-SUPERVISED TRAINING OF TEXTBASED REINFORCEMENT LEARNING AGENT WITH LARGE LANGUAGE MODELS,"Agent, Reinforcement-Learning",,,,,,,,,,,,173,,,,
InfiAgent: A Multi-Tool Agent for AI Operating Systems,Agent,,,,,,,,,,,InfiAgent,170,,,,
Predictive Minds: LLMs As Atypical Active Inference Agents,Agent,,,,,,,,,,,,169,,,,
Sparks of Artificial General Intelligence: Early experiments with GPT-4,"Benchmark, GPT4",,,,,,,,,,,,167,,,,
XAgent: An Autonomous Agent for Complex Task Solving,Agent,,,,,,,,,,,XAgent,164,,,,
Semantic HELM: A Human-Readable Memory for Reinforcement Learning,"Memory, Reinforcement-Learning",,,,,,,,,,,,154,,,,
AutoAgents: A Framework for Automatic Agent Generation,Agent,,,https://github.com/Link-AGI/AutoAgents,,,,,,,,AutoAgents,149,,,,
XAgent: An Autonomous Agent for Complex Task Solving,Agent,,https://blog.x-agent.net/blog/xagent/,,,,,,,,,XAgent,144,,,,
Communicative Agents for Software Development,"Agent, Soft-Dev",,,https://github.com/OpenBMB/ChatDev,,,,,,,,ChatDev,143,,,,
"A self-hosted, offline, ChatGPT-like chatbot, powered by Llama 2. 100% private, with no data leaving your device.","LLM, Open-source",,,https://github.com/getumbrel/llama-gpt,,,,,,,,llama-gpt,141,,,,
Look Before You Leap: Unveiling the Power ofGPT-4V in Robotic Vision-Language Planning,"Chain-of-Thought, GPT4, Reasoning, Robot",,https://robot-vila.github.io/ViLa.pdf,,,,,2023/11/29,,GPT4V,,,128,,,,
RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds,"Agent, Minecraft, Reinforcement-Learning",,,,,,,,,,Reinforcement Learning,RLAdapter,23,,,,